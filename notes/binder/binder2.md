#  摘要
Android 系统是基于 Linux  内核开发的。Binder 是 Android 系统中的进程间通信方式。

## 目前已有的 IPC(进程间通信) 方式。
- 管道
- system V IPC 
- socket


# 引言
CS（client-Server）模式广泛应用于各个领域，为了向应用开发者提供丰富多样的功能，这种通信方式更是无处不在，诸如媒体播放，视音频频捕获，到各种让手机更智能的传感器（加速度，方位，温度，光亮度等）都由不同的 Server 负责管理，应用程序只需做为 Client 与这些 Server 建立连接便可以使用这些服务，花很少的时间和精力就能开发出令人眩目的功能。

## 为什么采用 binder

1. 对于复杂性和可靠性而言。CS 广泛采用，对于 IPC 是个挑战，目前 Linux 支持的 IPC 中，只有 Socket 是支持 CS 的通信方式的，当然可以在已有的这些底层机制之上架设一套协议来实现 CS 通信，但是这样增加了系统的复杂性，可靠性也难以得到保证。

2. 对于传输的性能而言。Socket 作为一款通用的接口，传输效率低，开销大，主要用于跨网络的进程间通信和本机上进程间的低速通信。消息队列和管道采用存储-转发方式，即数据先发送方缓存区拷贝到内核开辟的缓存区中，然后再从内核缓存区拷贝到接收方缓存区，至少两次拷贝过程。共享内存虽然无需拷贝，但是控制复杂，难以使用。

共享内存->不想要内存拷贝。
Binder 有1次内存拷贝。
Socket/管道/消息队列， 有两次内存拷贝。

3. 从安全性考虑。Android 是个开放式的平台，应用来源广泛，确保智能终端的安全则是非常重要的。传统的 IPC 是无法获得对方进程可靠的 UID/PID (用户ID/进程ID)，从而无法鉴别对方的身份。 Android 为每个安装好的应用程序分配了自己的 UID，所以进程的 UID  是鉴别进程身份的重要标识。传统的 IPC 只能由用户阻塞数据包里填入 UID/PID，这样是不可靠的，容易被篡改。可靠的身份标记只能 IPC 机制本身在内核中添加。传统的 IPC 访问点都是开放的，无法建立私有的通道。比如命名管道的名称， system V 的键值， Socket 的 IP 地址或者文件名都是开放的，主要知道这些接入点的程序都可以和对端建立链接，不管怎样都无法阻止恶意程序通过猜测对方地址获得连接。

Binder 是基于 CS 通信模式的，传输过程中只需要一次拷贝，为发送方添加 UID/PID 身份，既支持实名 Binder 也支持匿名 Binder ，安全性高。


# Binder 的通信方式

Binder 使用 Client-Server 通信方式：1. 一个进程作为 Server 提供诸如视频/音频解码，视频捕获，地址本查询，网络连接等服务；2. 多个进程作为 Client 向 Server 发起服务请求，获得所需要的服务。
要想实现 Client-Server 通信据必须实现以下两点：
- 一是 Server 必须有确定的**访问接入点**或者说**地址**来接受 Client 的请求，并且 Client 可以通过某种途径获知 Server 的地址；
- 二是制定 Command-Reply 协议来传输数据。例如在网络通信中 Server 的访问接入点就是 Server 主机的IP地址+端口号，传输协议为 TCP 协议。对 Binder 而言，Binder 可以看成 Server 提供的实现某个特定服务的**访问接入点**， Client 通过这个*‘地址’*向 Server 发送请求来使用该服务；对 Client 而言，Binder 可以看成是通向 Server 的管道入口，要想和某个 Server 通信首先必须建立这个管道并获得管道入口。

# 面向对象的 Binder IPC

与其它 IPC 不同，Binder 使用了**面向对象**的思想来描述作为**访问接入点的 Binder**及其在 Client 中的**入口**：Binder 是一个实体位于 Server 中的对象，该对象提供了一套方法用以实现**对服务的请求**，就象类的成员函数。遍布于 Client 中的入口可以看成指向这个 Binder 对象的‘指针’，一旦获得了这个‘指针’就可以调用该对象的方法访问 Server 。在 Client 看来，通过 Binder ‘指针’调用其提供的方法和调用其它任何本地对象的方法并无区别，尽管 Binder 的实体位于远端 Server 中，而后者实体位于本地内存中。 ‘指针’是 C++ 的术语，而更通常的说法是引用，即 Client 通过 Binder 的引用访问 Server。而软件领域另一个术语‘句柄’也可以用来表述 Binder 在 Client 中的存在方式。从通信的角度看， Client 中的 Binder 也可以看作是 Server Binder 的‘代理’，在本地**代表远端 Server** 为 Client 提供服务。本文中会使用‘引用’或‘句柄’这个两广泛使用的术语。

面向对象思想的引入将进程间通信转化为通过对某个 Binder 对象的引用调用该对象的方法，而其独特之处在于 Binder 对象是一个可以**跨进程引用**的对象，**它的实体位于一个进程中，而它的引用却遍布于系统的各个进程之中。** 最诱人的是，这个引用和Java里引用一样既可以是强类型，也可以是弱类型，而且可以从一个进程传给其它进程，让大家都能访问同一 Server，就象将一个对象或引用赋值给另一个引用一样。Binder **模糊了进程边界**，淡化了进程间通信过程，整个系统仿佛运行于同一个面向对象的程序之中。形形色色的Binder 对象以及星罗棋布的引用仿佛粘接各个应用程序的胶水，这也是 Binder 在英文里的原意。

当然面向对象只是针对应用程序而言，对于 Binder 驱动和内核其它模块一样使用 C 语言实现，没有类和对象的概念。**Binder 驱动** 为面向对象的进程间通信提供底层支持。


# Binder 通信模型
Binder 框架定义了四个角色：Server，Client，ServiceManager（以后简称SMgr）以及 Binder 驱动。
其中 Server，Client，SMgr 运行于用户空间，驱动运行于内核空间。这四个角色的关系和互联网类似：Server是服务器，Client 是客户终端，SMgr 是域名服务器（DNS），驱动是路由器。

## Binder 驱动

## ServiceManager 与实名 Binder


## Client 获得实名 Binder 的引用

## 匿名 Binder


# Binder 协议
Binder 协议基本格式是（命令+数据），使用 ioctl(fd, cmd, arg) 函数实现交互。命令由参数 cmd 承载，数据由参数 arg 承载，随 cmd 不同而不同。

## Binder 常用通信命令
- BINDER_WRITE_READ
- BINDER_SET_MAX_THREADS
- BINDER_SET_CONTEXT_MGR
- BINDER_THREAD_EXIT
- BINDER_VERSION

BINDER_WRITE_READ。该命令的参数包括两部分数据：一部分是向Binder写入的数据，一部分是要从Binder读出的数据，驱动程序先处理写部分再处理读部分。这样安排的好处是应用程序可以很灵活地处理命令的同步或异步。例如若要发送异步命令可以只填入写部分而将read_size置成0；若要只从Binder获得数据可以将写部分置空即write_size置成0；若要发送请求并同步等待返回数据可以将两部分都置上。

## 写操作
最常用的是BC_TRANSACTION/BC_REPLY命令对，Binder请求和应答数据就是通过这对命令发送给接收方。这对命令所承载的数据包由结构体struct binder_transaction_data定义。Binder交互有同步和异步之分，利用binder_transaction_data中flag域区分。如果flag域的TF_ONE_WAY位为1则为异步交互，即Client端发送完请求交互即结束， Server端不再返回BC_REPLY数据包；否则Server会返回BC_REPLY数据包，Client端必须等待接收完该数据包方才完成一次交互。

## 从 Binder 读出数据
最重要的消息是BR_TRANSACTION 或 BR_REPLY，表明收到了一个格式为binder_transaction_data的请求数据包（BR_TRANSACTION）或返回数据包（BR_REPLY）。

## 收发数据包结构
Binder 收发数据包结构：binder_transaction_data

强调一下 offsets_size 和 data.offsets 两个成员：
如前述，Binder采用面向对象的设计思想，一个Binder实体可以发送给其它进程从而建立许多跨进程的引用；另外这些引用也可以在进程之间传递，就象java里将一个引用赋给另一个引用一样。

为 Binder 在**不同进程中建立引用**必须有驱动的参与，由驱动在**内核**创建并注册相关的数据结构后接收方才能使用该引用。而且这些引用可以是强类型，需要驱动为其维护引用计数。然而这些**跨进程传递的Binder混杂在应用程序发送的数据包里**，数据格式由用户定义，如果不把它们一一标记出来告知驱动，驱动将无法从数据中将它们提取出来。于是就使用 数组data.offsets 存放用户数据中每个 Binder 相对 data.buffer 的偏移量，用 offsets_size 表示这个数组的大小。驱动在发送数据包时会根据 data.offsets 和 offset_size 将散落于 data.buffer 中的 Binder 找出来并一一为它们创建相关的数据结构。在数据包中传输的 Binder 是类型为 struct flat_binder_object 的结构体。

对于接收方来说，该结构只相当于一个定长的消息头，真正的用户数据存放在 data.buffer 所指向的缓存区中。如果发送方在数据中内嵌了一个或多个 Binder ，**接收到的数据包**中同样会用 data.offsets 和 offset_size 指出每个 Binder 的位置和总个数。不过通常接收方可以忽略这些信息，因为接收方是知道数据格式的，参考双方约定的格式定义就能知道这些Binder在什么位置。


# Binder 的表述
Binder存在于系统以下几个部分中：

- 应用程序进程：分别位于 Server 进程和 Client 进程中
- Binder 驱动：分别管理为 Server 端的 Binder 实体和 Client 端的引用
- 传输数据：由于 Binder 可以跨进程传递，需要在传输数据中予以表述

## 在应用程序中的表述

Binder 本质上只是一种**底层通信方式**，和具体服务没有关系。为了提供具体服务，Server 必须提供一套接口函数以便 Client 通过远程访问使用各种服务。这时通常采用 Proxy 设计模式： 将接口函数定义在一个抽象类中， Server 和 Client 都会以该抽象类为基类实现所有接口函数，所不同的是 Server 端是真正的功能实现，而 Client 端是对这些函数远程调用请求的包装。如何将 Binder 和 Proxy 设计模式结合起来是应用程序实现面向对象 Binder 通信的根本问题。

### 在 Server 端的表述 – Binder实体

- 作为代理模式的基础,需要一个抽象接口封装 Server 所有功能,其中包含一些列抽象函数,留待 Server 和代理各自实现.
- 由于函数需要跨进程调用,需要为其一一编号,从而 Server 可以根据收到的编号决定调用哪个函数.
- Binder ,Server 端定义另一个 Binder 抽象类处理来自 Client 的 Binder 请求数据包,其中最重要的成员是抽象函数 onTransact . 该函数分析收到的数据包,调用响应的接口处理请求.

在 server 中以继承的方式,构建 Binder 的实体,实现所有的抽象函数,最重要的是 onTransact 函数,这个函数的输入是来自于 client 的 binder_transaction_data 结构的数据包。有个成员是 code ,包含了这次请求的接口函数编号.onTransact 将 case-by-case 地解析 code 值,从数据包取出函数参数,调用接口类中定义的函数(已被server 端实现).函数执行结束,如该需要返回值,就再构建一个 binder_transaction_data 包将返回数据包填入其中.

> 那么各个 Binder 实体的 onTransact 又是什么时候调用呢？这就需要驱动参与了。前面说过，Binder 实体须要以 Binder 传输结构 flat_binder_object 形式发送给其它进程才能建立 Binder 通信，而 Binder 实体指针就存放在该结构的 handle 域中。驱动根据 Binder 位置数组从传输数据中获取该 Binder 的传输结构，为它创建位于**内核中的 Binder 节点**，将 **Binder 实体指针**记录在该节点中。如果接下来有其它进程向该 Binder 发送数据，驱动会根据节点中记录的信息将 Binder 实体指针填入 binder_transaction_data 的 target.ptr 中返回给接收线程。接收线程从数据包中取出该指针， reinterpret_cast 成 Binder 抽象类并调用 onTransact() 函数。由于这是个虚函数，不同的 Binder 实体中有各自的实现，从而可以调用到不同 Binder 实体提供的 onTransact()。

### 在Client端的表述 – Binder引用
client 端的 binder 同样需要继承 server 提供的公共接口类,并实现公共函数.但这不是真正的实现,而是对远程函数调用的包装.将函数参数打包,通过 binder 向 server 发送申请并等待返回值.为此 client 端的 binder 还需要知道 binder 实体的相关系,即对 binder 实体的引用.

>由于继承了同样的公共接口类，Client Binder 提供了与 Server Binder 一样的函数原型，使用户感觉不出 Server 是运行在本地还是远端。Client Binder中，公共接口函数的包装方式是：创建一个 binder_transaction_data 数据包，将其对应的编码填入 code 域，将调用该函数所需的参数填入 data.buffer 指向的缓存中，并指明数据包的目的地，那就是已经获得的对 Binder 实体的引用，填入数据包的 target.handle 中。注意这里和 Server 的区别：实际上 target 域是个联合体，包括 ptr 和 handle 两个成员，前者用于接收数据包的 Server，指向 Binder 实体对应的**内存空间**；后者用于作为请求方的 Client，存放 Binder 实体的**引用**，告知驱动数据包将路由给哪个实体。数据包准备好后，通过驱动接口发送出去。经过 BC_TRANSACTION/BC_REPLY 回合完成函数的远程调用并得到返回值。


## Binder 在传输数据中的表述

Binder 可以塞在数据包的有效数据中越进程边界从一个进程传递给另一个进程，这些传输中的 Binder 用结构 flat_binder_object 表示.
无论是 Binder 实体还是对实体的引用都从属与某个进程，所以该结构不能透明地在进程之间传输，必须经过驱动翻译.

### flat_binder_object 成员

- type: 表明该 Binder 的类型: Binder 实体;Binder 的引用;文件形式的 Binder.
- flags: 该域只对第一次传递 Binder 实体时有效，因为此刻驱动需要在内核中创建相应的实体节点，有些参数需要从该域取出：处理本实体请求数据包的线程的最低优先级。表示该实体是否可以接收其它进程发过来的文件形式的 Binder .
- union {void *binder; signed long handle;} : 当传递的是 Binder 实体时使用 binder 域，指向 Binder 实体在应用程序中的地址。当传递的是 Binder 引用时使用 handle 域，存放 Binder 在进程中的引用号。
- cookie : 该域只对 Binder 实体有效，存放与该 Binder 有关的附加信息。

### 文件形式的 binder 

除了通常意义上用来通信的 Binder ，还有一种特殊的 Binder ：文件 Binder 。这种 Binder 的基本思想是：将文件看成 Binder 实体，进程打开的文件号看成 Binder 的引用。一个进程可以将它打开文件的文件号传递给另一个进程，从而另一个进程也打开了同一个文件，就象 Binder 的引用在进程之间传递一样。


>一个进程打开一个文件，就获得与该文件绑定的打开文件号。从Binder的角度，linux在内核创建的打开文件描述结构struct file是Binder的实体，打开文件号是该进程对该实体的引用。既然是Binder那么就可以在进程之间传递，故也可以用flat_binder_object结构将文件Binder通过数据包发送至其它进程，只是结构中type域的值为BINDER_TYPE_FD，表明该Binder是文件Binder。
而结构中的handle域则存放文件在发送方进程中的打开文件号。我们知道打开文件号是个局限于某个进程的值，一旦跨进程就没有意义了。这一点和Binder实体用户指针或Binder引用号是一样的，若要跨进程同样需要驱动做转换。驱动在接收Binder的进程空间创建一个新的打开文件号，将它与已有的打开文件描述结构struct file勾连上，从此该Binder实体又多了一个引用。新建的打开文件号覆盖flat_binder_object中原来的文件号交给接收进程。接收进程利用它可以执行read()，write()等文件操作


## Binder 在驱动中的表述

驱动是 Binder 通信的核心.
- 系统中所有的 Binder 实体以及每个实体在各个进程中的引用都登记在驱动中；
- 驱动需要记录 Binder 引用->实体之间多对一的关系；
- 为引用找到对应的实体；在某个进程中为实体创建或查找到对应的引用；
- 记录 Binder 的归属地（位于哪个进程中）；
- 通过管理 Binder 的强/弱引用创建/销毁 Binder 实体等等。

为了实现实名Binder的注册，系统必须创建第一只鸡–为 SMgr 创建的，用于注册实名 Binder 的 Binder 实体，负责实名 Binder 注册过程中的进程间通信。既然创建了实体就要有对应的引用：驱动将所有进程中的 0 号引用都预留给该 Binder 实体，即所有进程的 0 号引用天然地都指向**注册实名Binder专用的Binder**，无须特殊操作即可以使用 0 号引用来注册实名 Binder。接下来随着应用程序不断地注册实名 Binder ，不断向 SMgr 索要 Binder 的引用，不断将 Binder 从一个进程传递给另一个进程，越来越多的 Binder 以传输结构 - flat_binder_object的形式穿越驱动做跨进程的迁徙。由于 binder_transaction_data 中 data.offset 数组的存在，所有流经驱动的 Binder 都逃不过驱动的眼睛。Binder 将对这些穿越进程边界的 Binder 做如下操作：检查传输结构的 type 域，如果是 BINDER_TYPE_BINDER 或 BINDER_TYPE_WEAK_BINDER 则创建 Binder 的实体；如果是 BINDER_TYPE_HANDLE 或BINDER_TYPE_WEAK_HANDLE 则创建 Binder 的引用；如果是 BINDER_TYPE_HANDLE 则为进程打开文件，无须创建任何数据结构。随着越来越多的 Binder 实体或引用在进程间传递，驱动会在内核里创建越来越多的节点或引用，当然这个过程对用户来说是透明的。

### Binder 实体在驱动中的表述
驱动中的 Binder 实体也叫‘节点’，隶属于提供实体的进程，由 struct binder_node 结构来表示.

union {struct rb_node rb_node;struct hlist_node dead_node;}: 每个进程都维护一棵红黑树，以 Binder 实体在用户空间的指针，即本结构的 ptr 成员为索引存放该进程所有的 Binder 实体。这样驱动可以根据 Binder 实体在用户空间的指针很快找到其位于内核的节点。rb_node 用于将本节点链入该红黑树中。


>每个进程都有一棵红黑树用于存放创建好的节点，以 Binder 在用户空间的指针作为索引。每当在传输数据中侦测到一个代表 Binder 实体的 flat_binder_object ，先以该结构的 binder指针 为索引搜索红黑树；如果没找到就创建一个新节点添加到树中。由于对于同一个进程来说内存地址是唯一的，所以不会重复建设造成混乱。



### Binder 引用在驱动中的表述

Binder的引用也是驱动根据传输数据中的 flat_binder_object 创建的，隶属于获得该引用的进程，用 struct binder_ref 结构体表示：

就象一个对象有很多指针一样，同一个 Binder实体 可能有很多引用，不同的是这些引用可能分布在不同的进程中。和实体一样，每个进程使用红黑树存放所有正在使用的引用。不同的是 Binder 的引用可以通过两个键值索引：

- 对应实体在内核中的地址。注意这里指的是驱动创建于**内核**中的 binder_node结构 的地址，而不是 Binder实体 在**用户进程**中的地址。实体在内核中的地址是唯一的，用做索引不会产生二义性；但实体可能来自不同用户进程，而实体在不同用户进程中的地址可能重合，不能用来做索引。驱动利用该红黑树在一个进程中快速查找某个 Binder实体 所对应的引用（一个实体在一个进程中只建立一个引用）。

- 引用号。引用号是驱动为**引用**分配的一个32位标识，在一个**进程内是唯一的**，而在不同进程中可能会有同样的值，这和进程的打开文件号很类似。引用号将返回给应用程序，可以看作**Binder引用**在用户进程中的**句柄**。除了0号引用在所有进程里都固定保留给SMgr，其它值由驱动动态分配。向 Binder 发送数据包时，应用程序将**引用号**填入 binder_transaction_data结构的 target.handle域 中表明该数据包的 目的Binder 。驱动根据该引用号在红黑树中找到引用的 binder_ref结构 ，进而通过其 node域 知道目标 Binder实体 所在的进程及其它相关信息，实现数据包的路由。


# Binder 内存映射和接收缓存区管理

## 传统 IPC 进程间传递数据的方式
发送方将准备好的数据存放在缓存区中，调用API通过系统调用进入内核中。内核服务程序在内核空间分配内存，将数据从发送方缓存区复制到内核缓存区中。接收方读数据时也要提供一块缓存区，内核将数据从内核缓存区拷贝到接收方提供的缓存区中并唤醒接收线程，完成一次数据发送。

### 缺陷
- 浪费空间：首先是效率低下，需要做两次拷贝：用户空间->内核空间->用户空间。Linux 使用 copy_from_user() 和 copy_to_user() 实现这两个跨空间拷贝，在此过程中如果使用了高端内存（high memory），这种拷贝需要临时建立/取消页面映射，造成性能损失。
- 浪费时间：其次是接收数据的缓存要由接收方提供，可接收方不知道到底要多大的缓存才够用，只能开辟尽量大的空间或先调用 API 接收消息头获得消息体大小，再开辟适当的空间接收消息体。

## Binder 采用的方式 
Binder 采用一种全新策略：由 Binder驱动 负责管理数据接收缓存。我们注意到 Binder驱动 实现了 mmap() 系统调用，这对字符设备是比较特殊的，因为 mmap() 通常用在有物理存储介质的文件系统上，而象 Binder 这样没有物理介质，纯粹用来通信的字符设备没必要支持 mmap()。Binder驱动 当然不是为了在物理介质和用户空间做映射，而是用来创建数据接收的缓存空间。先看 mmap() 是如何使用的：

fd = open("/dev/binder", O_RDWR);

mmap(NULL, MAP_SIZE, PROT_READ, MAP_PRIVATE, fd, 0);

这样 Binder 的接收方就有了一片大小为 MAP_SIZE 的接受缓冲区 mmap() 的返回值是**内存映射在用户空间的地址**，不过这段空间是由驱动管理，用户不必也不能直接访问（映射类型为 PROT_READ ，只读映射）。

接收缓存区映射好后就可以做为缓存池接收和存放数据了。前面说过，接收数据包的结构为 inder_transaction_data，但这只是消息头，真正的有效负荷位于 data.buffer 所指向的内存中。这片内存不需要接收方提供，恰恰是来自mmap()映射的这片缓存池。在数据从发送方向接收方拷贝时，驱动会根据发送数据包的大小，使用最佳匹配算法从缓存池中找到一块大小合适的空间，将数据从发送缓存区复制过来。要注意的是，**存放 binder_transaction_data结构 本身以及表4中所有消息的内存空间还是得由接收者提供，但这些数据大小固定，数量也不多，不会给接收方造成不便**。映射的缓存池要足够大，因为接收方的线程池可能会同时处理多条并发的交互，每条交互都需要从缓存池中获取目的存储区，一旦缓存池耗竭将产生导致无法预期的后果。

有分配必然有释放。接收方在处理完数据包后，就要通知驱动释放 data.buffer 所指向的内存区。在介绍 Binder协议 时已经提到，这是由命令 BC_FREE_BUFFER 完成的。


通过上面介绍可以看到，驱动为接收方分担了最为繁琐的任务：分配/释放大小不等，难以预测的有效负荷缓存区，而接收方只需要提供缓存来存放大小固定，最大空间可以预测的消息头即可。在效率上，由于mmap()分配的内存是映射在接收方用户空间里的，所有总体效果就相当于对有效负荷数据做了一次从发送方用户空间到接收方用户空间的直接数据拷贝，省去了内核中暂存这个步骤，提升了一倍的性能。顺便再提一点，Linux内核实际上没有从一个用户空间到另一个用户空间直接拷贝的函数，需要先用copy_from_user()拷贝到内核空间，再用copy_to_user()拷贝到另一个用户空间。为了实现用户空间到用户空间的拷贝，mmap()分配的内存除了映射进了接收方进程里，还映射进了内核空间。所以调用copy_from_user()将数据拷贝进内核空间也相当于拷贝进了接收方的用户空间，这就是Binder只需一次拷贝的‘秘密’。

# Binder 接收线程管理
Binder通信实际上是位于**不同进程**中的**线程之间的通信**。假如 进程S 是 Server端，提供 Binder实体，线程T1 从Client 进程C1中 通过 Binder的引用 向 进程S 发送请求。S为了处理这个请求需要启动线程T2，而此时线程T1处于接收返回数据的等待状态。T2处理完请求就会将处理结果返回给T1，T1被唤醒得到处理结果。在这过程中，T2仿佛T1在进程S中的代理，代表T1执行远程任务，而给T1的感觉就是象穿越到S中执行一段代码又回到了C1。为了使这种穿越更加真实，驱动会将T1的一些属性赋给T2，特别是T1的优先级nice，这样T2会使用和T1类似的时间完成任务。很多资料会用‘线程迁移’来形容这种现象，容易让人产生误解。一来线程根本不可能在进程之间跳来跳去，二来T2除了和T1优先级一样，其它没有相同之处，包括身份，打开文件，栈大小，信号处理，私有数据等。

对于Server进程S，可能会有许多Client同时发起请求，为了提高效率往往开辟线程池并发处理收到的请求。怎样使用线程池实现并发处理呢？这和具体的IPC机制有关。拿socket举例，Server端的socket设置为侦听模式，有一个专门的线程使用该socket侦听来自Client的连接请求，即阻塞在accept()上。这个socket就象一只会生蛋的鸡，一旦收到来自Client的请求就会生一个蛋 – 创建新socket并从accept()返回。侦听线程从线程池中启动一个工作线程并将刚下的蛋交给该线程。后续业务处理就由该线程完成并通过这个单与Client实现交互。

可是对于Binder来说，既没有侦听模式也不会下蛋，怎样管理线程池呢？一种简单的做法是，不管三七二十一，先创建一堆线程，每个线程都用BINDER_WRITE_READ命令读Binder。这些线程会阻塞在驱动为该Binder设置的等待队列上，一旦有来自Client的数据驱动会从队列中唤醒一个线程来处理。这样做简单直观，省去了线程池，但一开始就创建一堆线程有点浪费资源。于是Binder协议引入了专门命令或消息帮助用户管理线程池，包括：

· INDER_SET_MAX_THREADS

· BC_REGISTER_LOOP

· BC_ENTER_LOOP

· BC_EXIT_LOOP

· BR_SPAWN_LOOPER

首先要管理线程池就要知道池子有多大，应用程序通过INDER_SET_MAX_THREADS告诉驱动最多可以创建几个线程。以后每个线程在创建，进入主循环，退出主循环时都要分别使用BC_REGISTER_LOOP，BC_ENTER_LOOP，BC_EXIT_LOOP告知驱动，以便驱动收集和记录当前线程池的状态。每当驱动接收完数据包返回读Binder的线程时，都要检查一下是不是已经没有闲置线程了。如果是，而且线程总数不会超出线程池最大线程数，就会在当前读出的数据包后面再追加一条BR_SPAWN_LOOPER消息，告诉用户线程即将不够用了，请再启动一些，否则下一个请求可能不能及时响应。新线程一启动又会通过BC_xxx_LOOP告知驱动更新状态。这样只要线程没有耗尽，总是有空闲线程在等待队列中随时待命，及时处理请求。

关于工作线程的启动，Binder驱动还做了一点小小的优化。当进程P1的线程T1向进程P2发送请求时，驱动会先查看一下线程T1是否也正在处理来自P2某个线程请求但尚未完成（没有发送回复）。这种情况通常发生在两个进程都有Binder实体并互相对发时请求时。假如驱动在进程P2中发现了这样的线程，比如说T2，就会要求T2来处理T1的这次请求。因为T2既然向T1发送了请求尚未得到返回包，说明T2肯定（或将会）阻塞在读取返回包的状态。这时候可以让T2顺便做点事情，总比等在那里闲着好。而且如果T2不是线程池中的线程还可以为线程池分担部分工作，减少线程池使用率。


# 数据包接收队列与（线程）等待队列管理

通常数据传输的接收端有两个队列：数据包接收队列和（线程）等待队列，用以缓解供需矛盾。当超市里的进货（数据包）太多，货物会堆积在仓库里；购物的人（线程）太多，会排队等待在收银台，道理是一样的。在驱动中，**每个进程**有一个全局的**接收队列**，也叫to-do队列，**存放不是发往特定线程的数据包**；相应地有一个**全局等待队列**，所有等待从全局接收队列里**收数据的线程**在该队列里排队。**每个线程有自己私有的to-do队列**，存放发送给该线程的数据包；相应的**每个线程都有各自私有等待队列**，专门用于本线程等待接收自己to-do队列里的数据。虽然名叫队列，其实线程私有等待队列中最多只有一个线程，即它自己。

由于发送时没有特别标记，驱动怎么判断哪些数据包该送入全局to-do队列，哪些数据包该送入特定线程的to-do队列呢？这里有两条规则。
- 规则1：Client发给Server的请求数据包都提交到Server进程的全局to-do队列。不过有个特例，就是上节谈到的Binder对工作线程启动的优化。经过优化，来自T1的请求不是提交给P2的全局to-do队列，而是送入了T2的私有to-do队列。(相互有binder实体交流)
- 规则2：对**同步请求**的返回数据包（由BC_REPLY发送的包）都发送到发起请求的线程的私有to-do队列中。如上面的例子，如果进程P1的线程T1发给进程P2的线程T2的是同步请求，那么T2返回的数据包将送进T1的私有to-do队列而不会提交到P1的全局to-do队列。

数据包进入接收队列的潜规则也就决定了线程进入等待队列的潜规则，即一个线程只要不接收返回数据包则应该在全局等待队列中等待新任务，否则就应该在其私有等待队列中等待Server的返回数据。还是上面的例子，T1在向T2发送同步请求后就必须等待在它私有等待队列中，而不是在P1的全局等待队列中排队，否则将得不到T2的返回的数据包。

这些潜规则是驱动对Binder通信双方施加的限制条件，体现在应用程序上就是同步请求交互过程中的线程一致性：
- 1) Client端，等待返回包的线程必须是发送请求的线程，而不能由一个线程发送请求包，另一个线程等待接收包，否则将收不到返回包；
- 2) Server端，发送对应返回数据包的线程必须是收到请求数据包的线程，否则返回的数据包将无法送交发送请求的线程。这是因为返回数据包的目的Binder不是用户指定的，而是驱动记录在收到请求数据包的线程里，如果发送返回包的线程不是收到请求包的线程驱动将无从知晓返回包将送往何处。

接下来探讨一下Binder驱动是如何递交同步交互和异步交互的。我们知道，同步交互和异步交互的区别是同步交互的请求端（client）在发出请求数据包后须要等待应答端（Server）的返回数据包，而异步交互的发送端发出请求数据包后交互即结束。对于这两种交互的请求数据包，驱动可以不管三七二十一，统统丢到接收端的to-do队列中一个个处理。但驱动并没有这样做，而是对异步交互做了限流，令其为同步交互让路，具体做法是：对于某个 Binder实体 ，只要有一个异步交互没有处理完毕，例如正在被某个线程处理或还在任意一条to-do队列中排队，那么接下来发给该实体的异步交互包将不再投递到to-do队列中，而是阻塞在驱动为该实体开辟的异步交互接收队列（Binder节点的async_todo域）中，但这期间同步交互依旧不受限制直接进入to-do队列获得处理。一直到该异步交互处理完毕，下一个异步交互方可以脱离异步交互队列进入to-do队列中。之所以要这么做是因为同步交互的请求端需要等待返回包，必须迅速处理完毕以免影响请求端的响应速度，而异步交互属于‘发射后不管’，稍微延时一点不会阻塞其它线程。所以用专门队列将过多的异步交互暂存起来，以免突发大量异步交互挤占Server端的处理能力或耗尽线程池里的线程，进而阻塞同步交互。




















# Binder 通信模型和 Binder 通信协议，了解 Binder 的设计需求

# Binder 在系统不同部分的 表述方式 和起的 作用

# Binder 在 数据接收端 的设计考虑，包括 线程池管理， 内存映射和等待队列管理等